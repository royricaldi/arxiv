{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Notes\n",
    "\n",
    "This notebook performs *web scraping* with Python for the arXiv.org website. It includes:\n",
    "- search by keywords\n",
    "- search by specific year\n",
    "directly \"asking\" the website. \n",
    "\n",
    "There is the use of the **arxiv** package available for Python which allows to perform a search by paper ID and getting information about the paper, such as the authors, title, publication date, etc. The paper ID can be found through web scraping of the arXiv.org website. \n",
    "\n",
    "The results of the search are:\n",
    "- titles\n",
    "- link for paper info\n",
    "- link for download pdf\n",
    "- link for donwload the source code folder\n",
    "\n",
    "which are stored in a *.csv* file at the end of the search. The notebook is interactive, then the user chooses the order to display results:\n",
    "- relevance\n",
    "- submission date (newest first)\n",
    "- submission date (oldest first)\n",
    "\n",
    "and what to download.\n",
    "\n",
    "The notebook also provides a *download* function.\n",
    "\n",
    "##### Update //TODOs\n",
    "\n",
    "Currently working on:\n",
    "- implement search which gives as results only papers with more versions already uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "from urllib import request\n",
    "import arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful global variables\n",
    "\n",
    "MAX_NUM_PER_PAGE = 50\n",
    "CURRENT_SIZE = 0\n",
    "REMAINING_ELEMENTS = 0\n",
    "TOTAL_RESULTS = 0\n",
    "TITLES = []\n",
    "LINK_PDFS = []\n",
    "LINK_INFO = []\n",
    "LINK_SOURCES = []\n",
    "PAPER_IDS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "website = \"https://arxiv.org/search/advanced\"\n",
    "\n",
    "# UNCOMMENT THE LINES BELOW IF YOU WANT THE WINDOW TO BE HIDDEN\n",
    "#options = webdriver.FirefoxOptions()\n",
    "#options.add_argument(\"--headless\")\n",
    "#driver = webdriver.Firefox(options = options)\n",
    "\n",
    "driver = webdriver.Firefox() \n",
    "driver.get(website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to interact with the user and start the search process\n",
    "\n",
    "def get_info_from_user():\n",
    "    terms = input(\"Enter keywords for your search: \")\n",
    "    q1 = input(\"\\nAre you looking for papers of a specific year? (y/n): \")\n",
    "    if q1 == \"y\":\n",
    "        year = input(\"Enter year (2007-YYYY): \")\n",
    "        if year.isdigit():\n",
    "            if int(year) >= 2007: \n",
    "                driver.find_element('xpath', '//input[@id=\"date-year\"]').send_keys(year)\n",
    "            else:\n",
    "                print(\"Try again\")\n",
    "                sys.exit(0)\n",
    "        else:\n",
    "            print(\"Try again\")\n",
    "            sys.exit(0)\n",
    "    else:\n",
    "        driver.find_element('xpath', '//input[@id=\"date-filter_by-0\"]').click()\n",
    "\n",
    "    driver.find_element('xpath', '//input[@id=\"terms-0-term\"]').send_keys(terms)\n",
    "    driver.find_element('xpath', '//button[@class=\"button is-link is-medium\"]').click()\n",
    "    select_order = input(\"\\nWhich order do you prefer? Choose an option: \\n1. Relevance\\n\" + \n",
    "                \"2. Submission Date (oldest first)\\n3. Submission Date (newest first) \\n\"\n",
    "                + \"Insert a number: \"\n",
    "                )\n",
    "    choose_order(select_order).click()\n",
    "    driver.find_element('xpath', '//button[@class=\"button is-small is-link\"]').click()\n",
    "\n",
    "def choose_order(select_order):\n",
    "    switcher = {\n",
    "        '1': driver.find_element('xpath', '//select[@id=\"order\"]/option[5]'),\n",
    "        '2': driver.find_element('xpath', '//select[@id=\"order\"]/option[4]'),\n",
    "        '3': driver.find_element('xpath', '//select[@id=\"order\"]/option[3]'),\n",
    "    }\n",
    "    return switcher.get(select_order)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell contains all the functions needed to manage pages.\n",
    "As default value, the maximum number of elements per page is 50. If the total number of elements for the whole search is greater than 50, then the user is asked if more results are needed; if so, the next page is loaded, otherwise all the results found up to that point are saved in the .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the size of the first page and extract all the relevant info of papers in it\n",
    "def first_page():\n",
    "    global CURRENT_SIZE\n",
    "    first_page_size()\n",
    "    extract_search_results(CURRENT_SIZE)\n",
    "\n",
    "# utility function to the get the size of the first page\n",
    "def first_page_size():\n",
    "    global CURRENT_SIZE\n",
    "    global REMAINING_ELEMENTS\n",
    "    list_size = (driver.find_element('xpath', '/html/body/main/div[1]/div[1]/h1').text).split(\" \")[-2]\n",
    "    if list_size.isdigit:\n",
    "        REMAINING_ELEMENTS = int(list_size)\n",
    "        get_size(REMAINING_ELEMENTS)\n",
    "    else:\n",
    "        CURRENT_SIZE = 0\n",
    "        print(\"No results found\")\n",
    "        sys.exit(0)\n",
    "\n",
    "# manage next pages and extract search results\n",
    "def next_page():\n",
    "    driver.find_element('xpath', '/html/body/main/div[2]/nav[1]/a[2]').click()\n",
    "    get_size(REMAINING_ELEMENTS)\n",
    "    extract_search_results(CURRENT_SIZE)\n",
    "\n",
    "def next_page_size():\n",
    "    get_size(REMAINING_ELEMENTS)\n",
    "\n",
    "# utility function to get the size of the current page\n",
    "def get_size(size):\n",
    "    global CURRENT_SIZE\n",
    "    global REMAINING_ELEMENTS\n",
    "    if size > MAX_NUM_PER_PAGE: \n",
    "        CURRENT_SIZE = MAX_NUM_PER_PAGE\n",
    "        REMAINING_ELEMENTS -= MAX_NUM_PER_PAGE\n",
    "    else:\n",
    "        # no more elements\n",
    "        CURRENT_SIZE = REMAINING_ELEMENTS\n",
    "        REMAINING_ELEMENTS = 0\n",
    "\n",
    "# extract titles, link for info, link to download pdf, link to download source code      \n",
    "def extract_search_results(size):\n",
    "    global TITLES\n",
    "    global LINK_PDFS\n",
    "    global LINK_INFO\n",
    "    global LINK_SOURCES\n",
    "    global TOTAL_RESULTS \n",
    "    global PAPER_IDS\n",
    "    TOTAL_RESULTS += size\n",
    "    for i in range(1, size+1):\n",
    "        url_xpath = '/html/body/main/div[2]/ol/li[' + str(i) +']/div/p/a'\n",
    "        paper_id = (driver.find_elements('xpath', url_xpath)[0].text).split(\":\")[-1]\n",
    "        search_paper = next(arxiv.Search(id_list=[paper_id]).results())\n",
    "        PAPER_IDS.append(paper_id)\n",
    "        TITLES.append(search_paper.title)\n",
    "        LINK_PDFS.append(\"https://arxiv.org/pdf/\" + paper_id + \".pdf\")\n",
    "        LINK_INFO.append(\"https://arxiv.org/abs/\" + paper_id)\n",
    "        LINK_SOURCES.append(\"https://arxiv.org/e-print/\" + paper_id)\n",
    "    print(\"\\nDone! \" + str(size) + \" results found.\")\n",
    "\n",
    "\n",
    "def ask_for_more():\n",
    "    q = input(\"\\nDo you want more results? (y/n): \") \n",
    "    if q == \"y\":\n",
    "        next_page()\n",
    "    else:\n",
    "        create_db()\n",
    "\n",
    "\n",
    "def create_db():\n",
    "    results_db = pd.DataFrame({'Paper ID': PAPER_IDS, 'Title': TITLES, 'Paper Info': LINK_INFO, \n",
    "                    'Link PDF': LINK_PDFS, 'Link Source': LINK_SOURCES})\n",
    "    results_db.to_csv('results.csv')\n",
    "    print(str(TOTAL_RESULTS) + \" results saved to results.csv\")\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_download():\n",
    "    to_print = input(\"How many files do you want to download? (1-\" + str(TOTAL_RESULTS) + \"): \")\n",
    "    q = input(\"\\nWhat do you want to download? (1/2/3)\\n\" + \"1. PDF version\\n2.\" +\n",
    "            \" Source code folder\\n3. Both (1/2/3)\\nInsert number: \")\n",
    "    if q == \"1\":\n",
    "        download_pdf(to_print)\n",
    "    elif q == \"2\":\n",
    "        download_source(to_print)\n",
    "    elif q == \"3\":\n",
    "        download_both(to_print)\n",
    "    else:\n",
    "        print(\"Try again\")\n",
    "        sys.exit(0)\n",
    "\n",
    "def download_pdf(to_print):\n",
    "    for i in range(1, int(to_print)+1):\n",
    "        pdf = request.urlretrieve(LINK_PDFS[i-1], PAPER_IDS[i-1] + \".pdf\")\n",
    "    print(\"Download complete!\")\n",
    "\n",
    "def download_source(to_print):\n",
    "    q = input(\"\\nDo you also want to extract the folder? (y/n): \")\n",
    "    if q == \"y\":\n",
    "        for i in range(1, int(to_print)+1):\n",
    "            source = request.urlretrieve(LINK_SOURCES[i-1], PAPER_IDS[i-1] + \".tar.gz\")\n",
    "            extract(source[0])\n",
    "        print(\"Download and extraction complete!\")\n",
    "    else:\n",
    "        for i in range(1, int(to_print)+1):\n",
    "            source = request.urlretrieve(LINK_SOURCES[i-1], PAPER_IDS[i-1] + \".tar.gz\")\n",
    "        print(\"Download complete!\")\n",
    "    \n",
    "def download_both(to_print):\n",
    "    for i in range(1, int(to_print)+1):\n",
    "        pdf = request.urlretrieve(LINK_PDFS[i-1], PAPER_IDS[i-1] + \".pdf\")\n",
    "        source = request.urlretrieve(LINK_SOURCES[i-1], PAPER_IDS[i-1] + \".tar.gz\")\n",
    "\n",
    "def extract(filename):\n",
    "    folder_name = filename.split(\".tar.gz\")[0]\n",
    "    with tarfile.open(filename, \"r:gz\") as tar:\n",
    "        tar.extractall(path = os.path.join(\"../jupyter\", folder_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    get_info_from_user()\n",
    "    first_page()\n",
    "    while REMAINING_ELEMENTS > 0:\n",
    "        ask_for_more()\n",
    "    create_db()\n",
    "    ask_download()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
