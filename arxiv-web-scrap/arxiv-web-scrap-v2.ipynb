{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Notes\n",
    "\n",
    "This notebook performs *web scraping* with Python for the arXiv.org website. It includes:\n",
    "- search by keywords\n",
    "- search by specific year\n",
    "directly \"asking\" the website. \n",
    "\n",
    "There is the use of the **arxiv** package available for Python which allows to perform a search by paper ID and getting information about the paper, such as the authors, title, publication date, etc. The paper ID can be found through web scraping of the arXiv.org website. \n",
    "\n",
    "The results of the search are:\n",
    "- titles\n",
    "- link for paper info\n",
    "- link for download pdf\n",
    "- link for donwload the source code folder\n",
    "\n",
    "which are stored in a *.csv* file at the end of the search, to keep track of the results. The notebook is interactive, then the user chooses the order to display results:\n",
    "- relevance\n",
    "- submission date (newest first)\n",
    "- submission date (oldest first)\n",
    "\n",
    "and what to download.\n",
    "\n",
    "The notebook also provides a *download* function.\n",
    "\n",
    "#### Updates \n",
    "\n",
    "The notebook has been updated (*v2*) and now includes:\n",
    "- a class **Search** to keep track of all the main information about the current search\n",
    "- a new type of search has been added, called **version_search()**\n",
    "\n",
    "Suppose you need papers with at max N versions, where the latest version has been published in the year YYYY, then the *version_search()* function allows to look for each paper that matches the above criteria.\n",
    "\n",
    "In this case, it is possible to download the latest version of each paper within a previous version (by default, the first version)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "from urllib import request\n",
    "import arxiv\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class search to create a search object to store the search information\n",
    "\n",
    "@dataclass\n",
    "class Search:\n",
    "    papers_id = []\n",
    "    titles = []\n",
    "    links_pdf = []\n",
    "    links_source = []\n",
    "    links_info = []\n",
    "    total_results = 0\n",
    "    remaining_results = 0\n",
    "\n",
    "# useful global variables\n",
    "\n",
    "OPTION_NUM_PER_PAGE = 1 # 1: 25, 2: 50, 3: 100, 4: 200  --> possible enumeration\n",
    "MAX_NUM_PER_PAGE = 25   # change in base of option_num_per_page\n",
    "\n",
    "CURRENT_SIZE = 0 # number of elements in current page\n",
    "GIVE_MORE_RESULTS = True\n",
    "PREV_VERSION = \"1\" # by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "website = \"https://arxiv.org/search/advanced\"\n",
    "\n",
    "# UNCOMMENT THE LINES BELOW IF YOU WANT THE WINDOW TO BE HIDDEN\n",
    "#options = webdriver.FirefoxOptions()\n",
    "#options.add_argument(\"--headless\")\n",
    "#driver = webdriver.Firefox(options = options)\n",
    "\n",
    "driver = webdriver.Firefox() \n",
    "driver.get(website)\n",
    "\n",
    "search = Search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to interact with the user and start the search process\n",
    "\n",
    "def start():\n",
    "    q = input(\"Do you want a (1/2):\\n1. Search by keywords\\n2. Search for papers with more versions\"\n",
    "                + \"\\nInsert a number: \")\n",
    "    if q == \"1\":\n",
    "        search.type = \"keywords\"\n",
    "        normal_search()\n",
    "    elif q == \"2\":\n",
    "        search.type = \"versions\"\n",
    "        versions_search()\n",
    "    else:\n",
    "        driver.quit()\n",
    "        sys.exit(\"Try again\")\n",
    "\n",
    "# search by keywords\n",
    "def normal_search():\n",
    "    terms = input(\"\\nEnter keywords for your search: \")\n",
    "    q1 = input(\"\\nAre you looking for papers of a specific year? (y/n): \")\n",
    "    if q1 == \"y\":\n",
    "        year = input(\"Enter year (2007-YYYY): \")\n",
    "        if year.isdigit():\n",
    "            if int(year) >= 2007: \n",
    "                driver.find_element('xpath', '//input[@id=\"date-year\"]').send_keys(year)\n",
    "            else:\n",
    "                driver.quit()\n",
    "                sys.exit(\"Try again\")\n",
    "        else:\n",
    "            driver.quit()\n",
    "            sys.exit(\"Try again\")\n",
    "    else:\n",
    "        driver.find_element('xpath', '//input[@id=\"date-filter_by-0\"]').click()\n",
    "\n",
    "    driver.find_element('xpath', '//input[@id=\"terms-0-term\"]').send_keys(terms)\n",
    "    driver.find_element('xpath', '//button[@class=\"button is-link is-medium\"]').click()\n",
    "\n",
    "    search.order = input(\"\\nWhich order do you prefer? Choose an option: \\n1. Relevance\\n\" + \n",
    "                    \"2. Submission Date (oldest first)\\n3. Submission Date (newest first) \\n\"\n",
    "                    + \"Insert a number: \")\n",
    "\n",
    "# search for versions\n",
    "def versions_search():\n",
    "    year = input(\"\\nEnter a year to start the search (YYYY): \")\n",
    "    version_max = input(\"Look for papers with a max number of versions (2-9): \")\n",
    "    \n",
    "    query = year[len(year) - 2:] # take last two digits\n",
    "    query += \"*v\" + version_max     # result: YY*vX \n",
    "\n",
    "    search.version_max = version_max\n",
    "\n",
    "    driver.find_element('xpath', '//input[@id=\"terms-0-term\"]').send_keys(query)\n",
    "    driver.find_element('xpath', '//select[@id=\"terms-0-field\"]/option[9]').click()\n",
    "    driver.find_element('xpath', '//button[@class=\"button is-link is-medium\"]').click()\n",
    "\n",
    "    search.order = input(\"\\nWhich order do you prefer? Choose an option: \\n1. Relevance\\n\" + \n",
    "                    \"2. Submission Date (oldest first)\\n3. Submission Date (newest first) \\n\"\n",
    "                    + \"Insert a number: \")\n",
    "\n",
    "# order the results by relevance or submission date\n",
    "def choose_order(select_order):\n",
    "    switcher = {\n",
    "        '1': driver.find_element('xpath', '//select[@id=\"order\"]/option[5]'),\n",
    "        '2': driver.find_element('xpath', '//select[@id=\"order\"]/option[4]'),\n",
    "        '3': driver.find_element('xpath', '//select[@id=\"order\"]/option[3]'),\n",
    "    }\n",
    "    return switcher.get(select_order)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell contains all the functions needed to manage pages.\n",
    "As default value, the maximum number of elements per page is 25. If the total number of elements for the whole search is greater than 25, then the user is asked if more results are needed; if so, the next page is loaded, otherwise all the results found up to that point are saved in the .csv file and then, the user is asked if the download is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the size of the first page and extract all the relevant info of papers in it\n",
    "def first_page():\n",
    "    list_size = (driver.find_element('xpath', '/html/body/main/div[1]/div[1]/h1').text).split(\" \")[-2]\n",
    "    list_size = list_size.replace(\",\", \"\")\n",
    "    if list_size.isdigit():\n",
    "        search.remaining_results = int(list_size) # the full list size\n",
    "        choose_order(search.order).click()\n",
    "        driver.find_element('xpath', '//button[@class=\"button is-small is-link\"]').click()\n",
    "        get_page_size()\n",
    "        extract_search_results(CURRENT_SIZE) # CURRENT SIZE initialized by get_page_size()\n",
    "    else:\n",
    "        driver.quit()\n",
    "        sys.exit(\"No results found! Try again.\")\n",
    "\n",
    "# utility function to get the size of the current page\n",
    "def get_page_size():\n",
    "    global CURRENT_SIZE, GIVE_MORE_RESULTS\n",
    "    driver.find_element('xpath', '//select[@id=\"size\"]/option[' + str(OPTION_NUM_PER_PAGE) + ']').click ()\n",
    "    driver.find_element('xpath', '//button[@class=\"button is-small is-link\"]').click()\n",
    "    if search.remaining_results > MAX_NUM_PER_PAGE: \n",
    "        CURRENT_SIZE = MAX_NUM_PER_PAGE\n",
    "        search.remaining_results -= MAX_NUM_PER_PAGE\n",
    "    else:\n",
    "        CURRENT_SIZE = search.remaining_results\n",
    "        # no more elements for next page\n",
    "        search.remaining_results = 0\n",
    "        GIVE_MORE_RESULTS = False\n",
    "\n",
    "# manage next pages and extract search results\n",
    "def next_page():\n",
    "    driver.find_element('xpath', '/html/body/main/div[2]/nav[1]/a[2]').click()\n",
    "    get_page_size()\n",
    "    extract_search_results(CURRENT_SIZE)\n",
    "\n",
    "# extract titles, link for info, link to download pdf, link to download source code      \n",
    "def extract_search_results(size):\n",
    "    search.total_results += size\n",
    "    for i in range(1, size+1):\n",
    "        url_xpath = '/html/body/main/div[2]/ol/li[' + str(i) +']/div/p/a'\n",
    "        paper_id = (driver.find_elements('xpath', url_xpath)[0].text).split(\":\")[-1]\n",
    "        search_paper = next(arxiv.Search(id_list=[paper_id]).results())        \n",
    "        search.papers_id.append(paper_id)\n",
    "        search.titles.append(search_paper.title)\n",
    "        search.links_pdf.append(\"https://arxiv.org/pdf/\" + paper_id + \".pdf\")\n",
    "        search.links_info.append(\"https://arxiv.org/abs/\" + paper_id)\n",
    "        search.links_source.append(\"https://arxiv.org/e-print/\" + paper_id)\n",
    "    print(\"\\nDone! \" + str(size) + \" results found.\")\n",
    "\n",
    "\n",
    "def ask_for_more():\n",
    "    global GIVE_MORE_RESULTS\n",
    "    q = input(\"\\nDo you want more results? (y/n): \") \n",
    "    if q == \"y\":\n",
    "        next_page()\n",
    "    else:\n",
    "        GIVE_MORE_RESULTS = False\n",
    "\n",
    "\n",
    "def create_db():\n",
    "    results_db = pd.DataFrame({'Paper ID': search.papers_id, 'Title': search.titles, 'Paper Info': search.links_info, \n",
    "                    'Link PDF': search.links_pdf, 'Link Source': search.links_source})\n",
    "    results_db.to_csv('results.csv')\n",
    "    print(str(search.total_results) + \" results saved to results.csv\")\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask for download and download function in base of the search.type (keywords or versions)\n",
    "def ask_download():\n",
    "    to_download = input(\"\\nDo you want to download the results? (y/n): \") \n",
    "    if to_download == \"y\":\n",
    "        to_print = input(\"\\nHow many files do you want to download? (1-\" + str(search.total_results) + \"): \")\n",
    "        q = input(\"\\nWhat do you want to download? (1/2/3)\\n\" + \"1. PDF version\\n2.\" +\n",
    "                    \" Source code folder\\n3. Both (1/2/3)\\nInsert number: \")\n",
    "        if search.type == \"versions\": \n",
    "            q2 = input(\"\\nDo you want to download also version n.\" + PREV_VERSION + \"? (y/n): \")\n",
    "            if q2 == \"y\":\n",
    "                if q == \"1\":\n",
    "                    download_both_pdf(to_print)\n",
    "                elif q == \"2\":\n",
    "                    download_both_source(to_print)\n",
    "                elif q == \"3\":\n",
    "                    download_prev_both(to_print)\n",
    "                else:\n",
    "                    sys.exit(\"Try again\")\n",
    "            else:\n",
    "                if q == \"1\":\n",
    "                    download_pdf(to_print)\n",
    "                elif q == \"2\":\n",
    "                    download_source(to_print)\n",
    "                elif q == \"3\":\n",
    "                    download_both(to_print)\n",
    "                else:\n",
    "                    sys.exit(\"Try again\")\n",
    "    else:\n",
    "        print(\"Bye!\")\n",
    "\n",
    "# functions to download in case of search.type == keywords\n",
    "def download_pdf(to_print):\n",
    "    for i in range(0, int(to_print)):\n",
    "        request.urlretrieve(search.links_pdf[i], search.papers_id[i] + \".pdf\")\n",
    "    print(\"Download complete!\")\n",
    "\n",
    "def download_source(to_print):\n",
    "    q = input(\"\\nDo you also want to extract the folder? (y/n): \")\n",
    "    if q == \"y\":\n",
    "        for i in range(0, int(to_print)):\n",
    "            source = request.urlretrieve(search.links_source[i], search.papers_id[i] + \".tar.gz\")\n",
    "            extract(source[0])\n",
    "        print(\"Download and extraction complete!\")\n",
    "    else:\n",
    "        for i in range(0, int(to_print)):\n",
    "            request.urlretrieve(search.links_source[i], search.papers_id[i] + \".tar.gz\")\n",
    "        print(\"Download complete!\")\n",
    "    \n",
    "def download_both(to_print):\n",
    "    for i in range(1, int(to_print)+1):\n",
    "        request.urlretrieve(search.links_pdf[i], search.papers_id[i] + \".pdf\")\n",
    "        request.urlretrieve(search.links_source[i], search.papers_id[i] + \".tar.gz\")\n",
    "\n",
    "\n",
    "# functions to download in case of search.type == versions\n",
    "# and download both previous and latest versions simultaneously\n",
    "def download_both_pdf(to_print):\n",
    "    for i in range(0, int(to_print)):\n",
    "        prev_paper_id = get_prev_id(i)\n",
    "        request.urlretrieve(search.links_pdf[i], search.papers_id[i] + \".pdf\")\n",
    "        request.urlretrieve(\"https://arxiv.org/pdf/\" + prev_paper_id + \".pdf\", prev_paper_id + \".pdf\")\n",
    "    print(\"Download complete!\")\n",
    "\n",
    "def download_both_source(to_print):\n",
    "    q = input(\"\\nDo you also want to extract the folder? (y/n): \")\n",
    "    if q == \"y\":\n",
    "        for i in range(0, int(to_print)):\n",
    "            prev_paper_id = get_prev_id(i)\n",
    "            source1 = request.urlretrieve(search.links_source[i], search.papers_id[i] + \".tar.gz\")\n",
    "            source2 = request.urlretrieve(\"https://arxiv.org/e-print/\" + prev_paper_id, prev_paper_id + \".tar.gz\")\n",
    "            extract(source1[0])\n",
    "            extract(source2[0])\n",
    "        print(\"Download and extraction complete!\")\n",
    "    else:\n",
    "        for i in range(0, int(to_print)):\n",
    "            prev_paper_id = get_prev_id(i)\n",
    "            source1 = request.urlretrieve(search.links_source[i], search.papers_id[i] + \".tar.gz\")\n",
    "            source2 = request.urlretrieve(\"https://arxiv.org/e-print/\" + prev_paper_id, prev_paper_id + \".tar.gz\")\n",
    "        print(\"Download complete!\")\n",
    "\n",
    "\n",
    "def download_prev_both(to_print):\n",
    "    for i in range(0, int(to_print)):\n",
    "        prev_paper_id = get_prev_id(i)\n",
    "        request.urlretrieve(\"https://arxiv.org/pdf/\" + prev_paper_id + \".pdf\", prev_paper_id + \".pdf\")\n",
    "        request.urlretrieve(\"https://arxiv.org/e-print/\" + prev_paper_id, prev_paper_id + \".tar.gz\")\n",
    "        request.urlretrieve(search.links_pdf[i], search.papers_id[i] + \".pdf\")\n",
    "        request.urlretrieve(search.links_source[i], search.papers_id[i] + \".tar.gz\")\n",
    "\n",
    "# other utility functions\n",
    "def extract(filename):\n",
    "    folder_name = filename.split(\".tar.gz\")[0]\n",
    "    with tarfile.open(filename, \"r:gz\") as tar:\n",
    "        tar.extractall(path = os.path.join(\"../jupyter\", folder_name))\n",
    "\n",
    "def get_prev_id(index):\n",
    "    paper_id = search.papers_id[index]\n",
    "    if paper_id[len(paper_id) - 2:].isdigit():  # no v in it\n",
    "        prev_paper_id = paper_id + \"v\" + PREV_VERSION\n",
    "        search.papers_id[index] += \"v\" + search.version_max\n",
    "    else:\n",
    "        prev_paper_id = paper_id.replace(\"v\" + search.version_max, \"v\" + PREV_VERSION)\n",
    "    return prev_paper_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done! 25 results found.\n",
      "25 results saved to results.csv\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elis/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3450: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# main\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start()\n",
    "    first_page()\n",
    "    while search.remaining_results > 0 and GIVE_MORE_RESULTS:\n",
    "        ask_for_more()\n",
    "    create_db()\n",
    "    ask_download()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
